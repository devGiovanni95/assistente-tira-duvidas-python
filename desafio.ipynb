{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #variavel global\n",
    "language='pt'\n",
    "# #Instalando whisper\n",
    "!pip install git+https://github.com/openai/whisper.git -q\n",
    "# #Instalando biblioteca da openai\n",
    "!pip install openai==0.28\n",
    "# #Biblioteca para sintetizar a voz\n",
    "!pip install gTTS\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Audio, display, Javascript\n",
    "from google.colab import output\n",
    "from base64 import b64decode\n",
    "from gtts import gTTS\n",
    "import openai\n",
    "import whisper\n",
    "\n",
    "\n",
    "gtts_object = gTTS(text='Bem vindo ao seu tira  dúvidas, qual a sua dúvida de hoje ?', lang=language, slow=False)\n",
    "gtts_welcome_path = \"/content/welcome.wav\"\n",
    "gtts_object.save(gtts_welcome_path)\n",
    "display(Audio(gtts_welcome_path, autoplay=True))  \n",
    "\n",
    "#Funcao em js on premisse onde habilita para escutar audios no navegador\n",
    "RECORD = \"\"\"\n",
    "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
    "const b2text = blob => new Promise(resolve => {\n",
    "  const reader = new FileReader()\n",
    "  reader.onloadend = e => resolve(e.srcElement.result)\n",
    "  reader.readAsDataURL(blob)\n",
    "})\n",
    "var record = time => new Promise(async resolve => {\n",
    "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "  recorder = new MediaRecorder(stream)\n",
    "  chunks = []\n",
    "  recorder.ondataavailable = e => chunks.push(e.data)\n",
    "  recorder.start()\n",
    "  await sleep(time)\n",
    "  recorder.onstop = async ()=>{\n",
    "    blob = new Blob(chunks)\n",
    "    text = await b2text(blob)\n",
    "    resolve(text)\n",
    "  }\n",
    "  recorder.stop()\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "def record(sec=10):\n",
    "    display(Javascript(RECORD))\n",
    "     #executar o codigo em js e formatando o paramentro s\n",
    "    js_result = output.eval_js('record(%s)' % (sec * 1000))\n",
    "    audio = b64decode(js_result.split(',')[1])\n",
    "    file_name = 'request_audio.wav'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(audio)\n",
    "    return f'/content/{file_name}'\n",
    "\n",
    "print('Ouvindo..... \\n')\n",
    "record_file = record()\n",
    "# display(Audio(record_file, autoplay=True))    \n",
    "display(Audio(record_file, autoplay=False))  \n",
    "\n",
    "\n",
    "gtts_object = gTTS(text='Certo, aguarde um momento', lang=language, slow=False)\n",
    "gtts_await_time_path = \"/content/gtts_await_time.wav\"\n",
    "gtts_object.save(gtts_await_time_path)\n",
    "display(Audio(gtts_await_time_path, autoplay=True))  \n",
    "\n",
    "\n",
    "\n",
    "#Pode ser o tiny small largel ou outro modelo \n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "#atribuindo ao result o audio gravado anteriormente e transcrevendo ele \n",
    "#FP16 = false por causa do tipo do audio que estamos utilizando e definimos qual o idioma \n",
    "result = model.transcribe(record_file, fp16=False, language=language)\n",
    "\n",
    "#criando uma nova variavel para receber o texto de result\n",
    "transcription = result[\"text\"]\n",
    "\n",
    "print('Escrevendo..... \\n')\n",
    "print(transcription)\n",
    "\n",
    "\n",
    "\n",
    "#definindo nossa chave de acesso\n",
    "openai.api_key = ''\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[ { \"role\": \"user\",\"content\": transcription } ]\n",
    ")\n",
    "\n",
    "chatgpt_response = response.choices[0].message.content\n",
    "print(chatgpt_response)\n",
    "\n",
    "\n",
    "\n",
    "gtts_object = gTTS(text='Pronto     ', lang=language, slow=False)\n",
    "gtts_await_time = \"/content/gtts_await_time.wav\"\n",
    "gtts_object.save(gtts_await_time)\n",
    "display(Audio(gtts_await_time, autoplay=True))  \n",
    "\n",
    "\n",
    "gtts_object = gTTS(text=chatgpt_response, lang=language, slow=False)\n",
    "response_audio = \"/content/response_audio.wav\"\n",
    "gtts_object.save(response_audio)\n",
    "display(Audio(response_audio, autoplay=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
